{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimas0824/Machine_Learning/blob/main/Jobsheet_13/ML_WEEK13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5vyf5wjWFY"
      },
      "source": [
        "# Praktikum 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nwTgfVtLi89R",
        "outputId": "e29e8f2c-a787-4766-b7e9-bbc5fed37daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.25491446917804494\n",
            "Epoch 1000, Loss: 0.20602602264180095\n",
            "Epoch 2000, Loss: 0.16742418599311107\n",
            "Epoch 3000, Loss: 0.09097069374601213\n",
            "Epoch 4000, Loss: 0.025433652379636476\n",
            "Epoch 5000, Loss: 0.0115355492288669\n",
            "Epoch 6000, Loss: 0.007023129122780075\n",
            "Epoch 7000, Loss: 0.0049360862072992395\n",
            "Epoch 8000, Loss: 0.003763875630922241\n",
            "Epoch 9000, Loss: 0.0030227066091228646\n",
            "Prediksi:\n",
            "[[0.04274377]\n",
            " [0.95222173]\n",
            " [0.95245898]\n",
            " [0.06077781]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2v9vxlGj3dH"
      },
      "source": [
        "## Tugas 1:\n",
        "\n",
        "* Ubah jumlah neuron hidden layer menjadi 3.\n",
        "\n",
        "* Bandingkan hasil loss dengan konfigurasi awal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hd4KqQ1JkBgn",
        "outputId": "3efa2bab-542e-44ca-c765-01a064ae5203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.267429293447181\n",
            "Epoch 1000, Loss: 0.2335236255859649\n",
            "Epoch 2000, Loss: 0.07240969364074616\n",
            "Epoch 3000, Loss: 0.017021822040387657\n",
            "Epoch 4000, Loss: 0.008065094005131578\n",
            "Epoch 5000, Loss: 0.005035932533295781\n",
            "Epoch 6000, Loss: 0.0035885835046952815\n",
            "Epoch 7000, Loss: 0.002758691713314847\n",
            "Epoch 8000, Loss: 0.002226763684095194\n",
            "Epoch 9000, Loss: 0.001859330592831659\n",
            "Prediksi:\n",
            "[[0.04428184]\n",
            " [0.96339008]\n",
            " [0.95782466]\n",
            " [0.03587744]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptc_c2I8kKgO"
      },
      "source": [
        "dari hasil diatas, model dengan menggunakan 2 hidden layer lebih stabil diawal tapi konvergensi lebih lambat daripada model 3 hidden layer. Sedangkan model dengan 3 hidden layer memiliki loss awal lebih tinggi daripada model 2 hidden layer, tapi setelah 2000 epoch performa model jauh lebih baik dan konvergen lebih cepat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-AaskUzkIq3"
      },
      "source": [
        "* Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HCkxQr_Ik8dD",
        "outputId": "12b778d3-c448-4a4d-bfc0-4052357cbd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.223214\n",
            "Epoch 1000, Loss: 0.126319\n",
            "Epoch 2000, Loss: 0.125457\n",
            "Epoch 3000, Loss: 0.125249\n",
            "Epoch 4000, Loss: 0.125171\n",
            "Epoch 5000, Loss: 0.125128\n",
            "Epoch 6000, Loss: 0.125107\n",
            "Epoch 7000, Loss: 0.125084\n",
            "Epoch 8000, Loss: 0.125072\n",
            "Epoch 9000, Loss: 0.125062\n",
            "\n",
            "Prediksi akhir:\n",
            "Input: [0 0] -> Target: 0 | Prediksi: 0.4999\n",
            "Input: [0 1] -> Target: 1 | Prediksi: 0.9906\n",
            "Input: [1 0] -> Target: 1 | Prediksi: 0.4999\n",
            "Input: [1 1] -> Target: 0 | Prediksi: 0.0121\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)   # gunakan ReLU di hidden layer\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)  # output tetap sigmoid\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "# Output akhir lebih rapi\n",
        "print(\"\\nPrediksi akhir:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]} -> Target: {y[i][0]} | Prediksi: {a2[i][0]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFFEdmcFlN0y"
      },
      "source": [
        "Model dengan tambahan activation ReLU, memiliki loss tidak turun mendekati nol, melainkan stagnan dan pletau di sekitar 0.125. Ini menunjukkan model tidak dapat belajar pola data XOR. Pada model tanpa ReLU, prediksi mendektai target dengan baik. Sedangkan dengan ReLU activation model juga hampir benar tapi memiliki kesalahan yang lebih tinggi daripada model sebelumnya."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO9AeBe5MlgI/sEHv2Kwc+j",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
